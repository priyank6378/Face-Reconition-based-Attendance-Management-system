{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cca7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import cv2, os, numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_face():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg_face()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fdc73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(r\"C:\\Users\\hp\\.deepface\\weights\\vgg_face_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face_descriptor = tf.keras.Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98db3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.4\n",
    "\n",
    "def distance(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation),test_representation)\n",
    "    b = np.sum(np.multiply(source_representation,source_representation))\n",
    "    c = np.sum(np.multiply(test_representation,test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "def verify_face(img1, img2):\n",
    "    img1_representation = vgg_face_descriptor.predict(np.array([img1]))[0,:]\n",
    "    img2_representation = vgg_face_descriptor.predict(np.array([img2]))[0,:]\n",
    "    cosine_similarity = distance(img1_representation,img2_representation)\n",
    "    \n",
    "    print(\"Cosine similarity: \",cosine_similarity)\n",
    "    \n",
    "    if(cosine_similarity < epsilon):\n",
    "        print(\"They are same person\")\n",
    "    else:\n",
    "        print(\"They are not same person!\")\n",
    "        \n",
    "def process_image(img1):\n",
    "    shape = (224,224)\n",
    "    i1 = cv2.imread(img1)[:,:,::-1]\n",
    "    i1 = cv2.resize(i1, shape)\n",
    "    return i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = r\"E:\\semester 5\\mini project\\face recognition algorithms\\dataset\\Tomar\\115.jpg\"\n",
    "img2 = r\"E:\\semester 5\\mini project\\face recognition algorithms\\dataset\\Tomar\\139.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (224,224)\n",
    "i1 = cv2.imread(img1)[:,:,::-1]\n",
    "i1 = cv2.resize(i1, shape)\n",
    "i2 = cv2.imread(img2)[:,:,::-1]\n",
    "i2 = cv2.resize(i2, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_face(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28c16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fcd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(base_dir = r\"F:\\mini project 1\\dataset\", make_np = True):\n",
    "    training_images_dir = [base_dir+'\\\\'+name for name in os.listdir(base_dir)]\n",
    "    names = {}\n",
    "    i = 0\n",
    "    for p in os.listdir(base_dir):\n",
    "        names[p] = i\n",
    "        i+=1\n",
    "    X = []\n",
    "    y = []\n",
    "    for person_dir in training_images_dir:\n",
    "        for img in os.listdir(person_dir):\n",
    "            image = process_image(person_dir+'\\\\'+img)\n",
    "            X.append(image)\n",
    "            y.append(names[person_dir.split('\\\\')[-1]])\n",
    "    if make_np:\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = make_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19152f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "base = r\"F:\\mini project 1\\dataset\"\n",
    "names = os.listdir(base)                # names of people\n",
    "\n",
    "person_face_encodings = dict()\n",
    "\n",
    "for name in names:\n",
    "    person_face_encodings[name] = []\n",
    "    for file_name in os.listdir(base+'/'+name):\n",
    "        img = process_image(base+'/'+name+'/'+file_name)\n",
    "        encodings = vgg_face_descriptor(np.array([img]))[0,:]\n",
    "        if len(encodings):\n",
    "            person_face_encodings[name].append(encodings)\n",
    "\n",
    "try :\n",
    "    file = open(\"./vgg_face_encoding.pickle\", 'wb')\n",
    "    pickle.dump(person_face_encodings, file)\n",
    "    file.close()\n",
    "except:\n",
    "    print(\"Something went wrong!\")\n",
    "    os._exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64614d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dict()\n",
    "\n",
    "a = 0\n",
    "for p in list(person_face_encodings.keys()):\n",
    "    target[p] = a\n",
    "    a += 1\n",
    "reverse_target = dict([(v,k) for k,v in target.items()])\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for k,v in person_face_encodings.items():\n",
    "    for i in v:\n",
    "        X.append(i)\n",
    "        y.append(target[k])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, reverse_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594214a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, n_jobs=-1)\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07379bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection_2(img):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray_img )\n",
    "    cropped_faces = []\n",
    "    take = 7  # increase the size of the bounding box\n",
    "    for face in faces:\n",
    "        cropped_faces.append(img[ face.top()-take :face.bottom()+take, face.left()-take:face.right()+take , :].copy())\n",
    "    for face in faces:\n",
    "        cv2.rectangle(img,(face.left()-5, face.top()-5), (face.right()+5, face.bottom()+5) ,(0,255,0), 5)\n",
    "    return cropped_faces , img\n",
    "\n",
    "def recognize_faces(img_path, shape = (224,224)):\n",
    "    try:\n",
    "        file = open(\"./face_encodings.pickle\", 'rb')\n",
    "        person_face_encodings = pickle.load(file)\n",
    "        file.close()\n",
    "    except:\n",
    "        print(\"Something went wrong!\")\n",
    "        os._exit(1)\n",
    "    img = cv2.imread(img_path)[:, : , ::-1]\n",
    "    cropped_faces, img = face_detection_2(img)\n",
    "    print(len(cropped_faces), cropped_faces[0].shape)\n",
    "    plt.imshow(img)\n",
    "\n",
    "    ans = dict()\n",
    "    face_vs_name = []\n",
    "    for i,face in enumerate(cropped_faces):\n",
    "        # plt.imshow(face)\n",
    "        # plt.show()\n",
    "        unknown_encoding = fr.face_encodings(face ,num_jitters=3 , model='large')\n",
    "        if len(unknown_encoding):\n",
    "            ans = knn.predict(unknown_encoding)\n",
    "            face_vs_name.append(reverse_target[ans[0]])\n",
    "        else :\n",
    "            face_vs_name.append(\"~ENCODING\")\n",
    "\n",
    "    fig, axes = plt.subplots( math.ceil(len(face_vs_name)/2) , 2 )\n",
    "    for i,face in enumerate(cropped_faces):\n",
    "        axes.ravel()[i].imshow(face)\n",
    "        axes.ravel()[i].set_title(face_vs_name[i])\n",
    "        axes.ravel()[i].axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return face_vs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5abcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(\"./vggface_knn.pickle\", 'wb')\n",
    "    pickle.dump(knn, f)\n",
    "    f.close()\n",
    "except:\n",
    "    print(\"Something went wrong!\")\n",
    "    os._exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e3ea3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9aed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(\"./facenet_knn.pickle\", 'rb')\n",
    "    model = pickle.load(f)\n",
    "    f.close()\n",
    "except:\n",
    "    print(\"Something went wrong!\")\n",
    "    os._exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c9e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_faces(r\"F:\\mini project 1\\test images\\1.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d74903f3c0a8ddc14f9ca93275717badad87b72706d588d6160414d0927f0c5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
